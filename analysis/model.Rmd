---
title: "Rare mutation detection model"
output:
  workflowr::wflow_html:
      code_folding: hide
---

```{r knitr, include = FALSE}
DOCNAME = "Simulation_Benchmarking"
knitr::opts_chunk$set(autodep        = TRUE,
                      cache          = FALSE,
                      cache.path     = paste0("cache/", DOCNAME, "/"),
                      cache.comments = FALSE,
                      echo           = TRUE,
                      error          = FALSE,
                      fig.align      = "center",
                      fig.width      = 9,
                      fig.height     = 5,
                      dev            = c("png"),
                      message        = FALSE,
                      warning        = FALSE)
```

```{r libraries, cache = FALSE}
library(ggplot2)
library(dplyr)
```

# Model

Q: how many cells (or "cell equivalents") do we need to sequence to detect variants down to a VAF of X%

## Assumptions

* We consider a haploid genome without SCNAs
* We're not considering DNA extraction efficiency or ligation efficiency

## Probability of sequencing a mutant cell

The probability of selecting a cell with a given variant from a pool of $n$ cells is the variant allele frequency (VAF) $v$. This indicates that a VAF of 0.01 means 1 cell in 100 is a mutant ($nv$). 

We assume the probability of selecting a mutant cell is binomially distributed. We want to know the probability of selecting at least one mutant:

$P(Bin(n, v)) > 0)$ = 0.95

This is equivalent to:

$P(Bin(n, v)) = 0)$ = 0.05

Where $n$ is the number of sequenced cells.

Let's consider how many input cells ($n$) are required to select at least one mutant cell at 1% VAF (95% confidence).

We'll consider 100 cell increments from $n = \{100, 200..2000\}$. Using these values, we can plot the probability selecting of sequencing at least one cell.

```{r cells_plot}
v = 0.01
n = seq(100, 2000, 100)

vafs <- data.frame(vaf=v,
                   p=pbinom(0, n, v),
                   input_cells=n,
                   mutant_cells=(n * v))

ggplot(vafs, aes(input_cells, p)) +
    geom_point() +
    theme_bw() +
    geom_hline(yintercept=0.05, alpha=0.4)
```

We can also plot this as mutant cells instead of VAF:

```{r mutant_cells_plot}
ggplot(vafs, aes(mutant_cells, p)) +
    geom_point() +
    theme_bw() +
    geom_hline(yintercept=0.05, alpha=0.4)
```

This shows that for a VAF target of 1%, we should sequence at least 300 cells (3 are mutants).

```{r cells_at_0.05}
deviation <- abs(0.05 - vafs$p)
print(vafs[which(deviation == min(deviation)),])
```

## Varying the VAF

If we select 1000 input cells, what's the lowest VAF we can sequence down to, where $v = {0.0002, 0.0004..0.01}$.

```{r varying_vaf}
v = seq(0.0002, 0.01, 0.0002)
n = 1000

vafs <- data.frame(vaf=v,
                   p=pbinom(0, n, v),
                   input_cells=n,
                   mutant_cells=(n * v))

ggplot(vafs, aes(vaf, p)) +
    geom_point() +
    theme_bw() +
    geom_hline(yintercept=0.05, alpha=0.4)
```

We can sequence down to approx 0.3% VAF. 

```{r vaf_at_1k_cells}
deviation <- abs(0.05 - vafs$p)
print(vafs[which(deviation == min(deviation)),])
```

We can look at the relationship between input cells and allele frequency: 

```{r cells_vs_vaf_plot, fig.height=6.5, fig.width=9}
cells_vs_vaf = NULL
n = seq(100, 10000, 100)
V = seq(0.001, 0.01, 0.001)
for (v in V) {
    toadd <- data.frame(
        vaf=as.factor(v),
        p=pbinom(0, n, v),
        total_cells=n
    )
    cells_vs_vaf <- rbind(cells_vs_vaf, toadd)
}

ggplot(cells_vs_vaf, aes(total_cells, p, colour=vaf)) +
    geom_line() +
    theme_bw() +
    theme(legend.position = 'bottom') +
    geom_hline(yintercept=0.05, alpha=0.4)
```

We can define an equation based on the binomial probability calculation, of obtaining the number of cells to sequence, to be 95% confident of sequencing the mutation based on the target VAF:

$(1 â€“ v) ^ n = 0.05$

## Coverage

The above thought experiment won't tell us the probability of sequencing the variant, as sequencing involves a whole host of other factors. Coverage is one of the most important. Given that approximately 30x raw coverage is required to yield 1x of high quality duplex coverage, we can look at the relationship between coverage and VAF (need the variant to be at least 1x to detect).

Here we can see that at VAF = 0.01 (line), we need to sequence to approx 3000x, and this gets exponentially deeper as we go down in target VAF.

```{r coverage}
v = seq(0.001, 0.1, 0.001)
ccov <- data.frame(d = 30 / v,
                   v = v)
ggplot(ccov, aes(d, v)) +
    geom_line() +
    theme_bw() +
    geom_hline(yintercept=0.01, alpha=0.4)
```
## Mixture experiments

How much total coverage will we need, in order to detect a mixture of $X\%?$ We define this as successfully calling at least 50\% of the variants at VAF $v = \{0.1, 0.01, 0.001\}$ with at least one supporting duplex read. We assume our duplex yield is 30 (i.e., 30x coverage yields 1x duplex coverage).

Our binomial calculation takes the form of:

$P(Bin(n, v)) = 0)$ = 0.5 where $n =$ duplex coverage.

```{r mixture_variant_calling}
df <- NULL
min_reads <- 1
duplex_yield <- 30
duplex_seq_range <- seq(1, 10000, 1)
fraction_we_can_miss <- 0.5

for(vaf in c(0.1, 0.01, 0.001)) {
    tmp <- data.frame(
        pval=pbinom(min_reads - 1, duplex_seq_range, vaf),
        vaf=vaf,
        duplex_cov=duplex_seq_range,
        total_cov=duplex_seq_range * duplex_yield
    )
    tmp <- tmp[tmp$pval < fraction_we_can_miss,]
    tmp <- tmp[tmp$pval == max(tmp$pval),]
    df <- rbind(df, tmp)
}

print(df)
```

## Revised model

Attempting to combine both coverage and cell input, we can take the following formula:

$(1 - v) ^ {(n * c * d)} = p$

Where $v$ = target VAF, $n$ = number of input cells, $c$ = coverage per input cell, $d$ = duplex efficiency and $p$ = probability of missing a variant.

We set:

* $c = 10$ (coverage we are aiming for for each genome equivalent)
* $d = 0.057$ (average efficiency for NanoSeq MB2)
* $p = 0.05$ (fraction of variants we are okay to miss)
* $v = \{0.001, 0.01, 0.05, 0.1\}$ (target VAFs)
* $n = \{100, 200..10000\}$ (range of input cells)

We also need to check whether the number of genome equivalents we take forward for sequencing exceeds the lane capacity (600GB). Our sequenced bases will be:

$b = g * c * n$ where $g$ = genome size. 

```{r revised_model}
lane_cap <- 6e11
g <- 5e6
pmiss <- 0.05
c <- 8
e <- 1/ 30

vafs <- c(0.001, 0.01, 0.05, 0.1)
n <- seq(100, 12000, 100)

# calculate duplex coverage
dcov <- n * c * e

# calculate total sequenced bases
bases <- g * c * n

probs <- NULL
min_cells <- NULL
for(vaf in vafs) {
    prob <- data.frame(ncells = n,
                        dcov = dcov,
                        p = pbinom(0, round(dcov), vaf),
                        vaf = vaf,
                        bases = bases,
                        within_capacity = bases < lane_cap)
    
    tmp <- prob[prob$p < pmiss,]
    tmp <- tmp[tmp$p == max(tmp$p),]
    min_cells <- rbind(tmp, min_cells)

    probs <- rbind(probs, prob)
}

print(min_cells)
ggplot(probs, aes(ncells, p, colour = factor(vaf))) +
    geom_point() +
    theme_minimal() +
    geom_hline(yintercept = pmiss, alpha=0.4) +
    scale_color_brewer(palette = 'Dark2')
```

## Sequencing depth and capture size

Example statistics table for 1% and 0.1% VAF experiments for different capture sizes (2kb, 10kb, 5Mb and 36.8Mb (human exome size).

```{r depth_and_size}
lane_cap <- 6e11
capture_sizes <- c(2000, 10000, 5e6, 3.68e7, 4e7)
chance_to_miss <- 0.05
target_cov <- 8
efficiency <- 1 / 30 # conservative estimate
ploidy <- 2
readlen <- 300
ncells <- c(5, seq(200, 6000, 10))
vafs <- c(0.01, 0.05, 0.1)

df_full <- NULL
for (capture_size in capture_sizes) {
    for (vaf in vafs) {
        df <- NULL
        for (ncell in ncells) {
            dna_depth <- ncell * ploidy
            dna_input <- capture_size * dna_depth
            seq_bases <- dna_input * target_cov
            tot_cov <- dna_depth * target_cov
            dup_cov <- tot_cov * efficiency
            tmp <- data.frame(ncell = ncell,
                              vaf = vaf,
                              tot_cov = tot_cov,
                              dup_cov = dup_cov,
                              seq_megabases = seq_bases / 1e6,
                              million_reads = seq_bases / readlen / 1e6,
                              capture_size_kb = capture_size / 1e3,
                              chance_to_miss = pbinom(0, round(dup_cov), vaf),
                              lane_fraction = seq_bases / lane_cap,
                              max_samples_per_lane = as.integer(1 / (seq_bases / lane_cap)))
            df <- rbind(df, tmp)
        }
        df <- df[df$chance_to_miss < chance_to_miss,]
        df <- df[df$chance_to_miss == max(df$chance_to_miss),]
        df_full <- rbind(df_full, df)
    }
}
print(df_full[order(df_full$vaf, decreasing = TRUE),])
```


## Downsampling experiment model

How does the duplicate rate behave when we downsample reads? We can use the rough stats of the 1\% ecoli spike-in to check for sensible downsampling fractions. This sample has roughly 340m duplex reads and an average family size of 14 (each read is sequenced on average 14x). We use a poisson distribution to generate fake reads, and then down sample from 5% to 95%. We then calculate the duplicate rate.

```{r downsampling_duplicates}
N_READS <- 340
COV_RATIO <- 14

# recursive function, times = number of letters in toy reads
make_toy_reads <- function(times, toy_reads=NULL) {
    if (times < 1) {
        return(toy_reads)
    } else if (is.null(toy_reads)) {
        toy_reads <- LETTERS
        times <- times - 1
    } else {
        toy_reads <- lapply(toy_reads, paste0, LETTERS) %>% unlist()
        times <- times - 1
    }
    make_toy_reads(times, toy_reads)
}

toy_reads <- make_toy_reads(2) %>% head(N_READS)
all_reads <- rep(toy_reads, rpois(N_READS, COV_RATIO))
dup_rate <- 1 - (N_READS / length(all_reads))
print(paste('Max duplicate rate:', round(dup_rate, 2)))

sample_frac <- seq(0.05, 1, 0.05)
n_sample <- round(sample_frac * length(all_reads))

all_samples <- lapply(n_sample, function(n){sample(all_reads, n)})
dup_rates <- lapply(all_samples, function(s){1 - (length(unique(s)) / length(s))}) %>% unlist()

df <- data.frame(sample_frac, dup_rates)
ggplot(df, aes(sample_frac, dup_rates)) +
    geom_point() +
    geom_line() +
    theme_minimal() +
    scale_x_continuous(breaks = seq(0.1, 1, 0.1)) +
    scale_y_continuous(breaks = seq(0.1, 1, 0.1))
```

